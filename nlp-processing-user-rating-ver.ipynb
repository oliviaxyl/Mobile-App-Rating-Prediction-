{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm.auto import tqdm\n",
    "import string\n",
    "import math\n",
    "import operator\n",
    "import pkg_resources\n",
    "from pyphen import Pyphen\n",
    "\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Example\n",
    "import torchtext\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Flatten, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, mean_squared_error\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try? the word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70 # max number of words in a question to use\n",
    "batch_size = 512 # how many samples to process at once\n",
    "n_epochs = 40 # how many times to iterate over all samples\n",
    "n_splits = 5 # Number of K-fold Splits\n",
    "SEED = 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cores:  2\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "from multiprocessing import Pool\n",
    "num_partitions = 8 # number of partitions to split dataframe\n",
    "num_cores = psutil.cpu_count() # number of cores on your machine\n",
    "print('number of cores: ', num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run parallelly\n",
    "def df_parallelize_run(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed = 1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        if s in text:\n",
    "            text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in text:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        if p in text:\n",
    "            text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        if p in text:\n",
    "            text = text.replace(p, ' ' + str(p) + ' ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        if s in text:\n",
    "            text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(\"Added \" + str(count) + \" words to embedding\")   \n",
    "    \n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' ' + str(punct) + ' ')\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "#############################################\n",
    "# add features in the datasets\n",
    "#############################################\n",
    "def add_features(df):\n",
    "    df.loc[:, 'app_desc'] = df.loc[:,'app_desc'].progress_apply(lambda x:str(x))\n",
    "    df.loc[:,'total_length'] = df.loc[:,'app_desc'].progress_apply(len)\n",
    "    df.loc[:,'capitals'] = df.loc[:,'app_desc'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df.loc[:,'caps_vs_length'] = df.progress_apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df.loc[:,'num_words'] = df.app_desc.str.count('\\S+')\n",
    "    df.loc[:,'num_unique_words'] = df.loc[:,'app_desc'].progress_apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df.loc[:,'words_vs_unique'] = df.loc[:,'num_unique_words'] / df['num_words']  \n",
    "    df.loc[:,'size_bytes_in_MB'] = df.loc[:,'size_bytes'] / (1024 * 1024.0)\n",
    "    df.loc[:,'isNotFree'] = df.loc[:,'price'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df.loc[:,'isNotGame'] = df.loc[:,'prime_genre'].apply(lambda x : 1 if x == 'Games' else 0)\n",
    "    feature_list = ['total_length', 'num_words', 'num_unique_words', 'caps_vs_length', 'size_bytes_in_MB', 'isNotGame', 'isNotFree']\n",
    "    return df, feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quora-insincere-questions-classification', 'app-store-apple-data-set-10k-apps']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('../input'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7154873b0688413d8a0ebbaf13afe1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=5754, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a144c3c292ff45cc89e9ffaf59e2de5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=5754, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d7d836bc384c3a8a8e82515da896c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=5754, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f222a232fa42fabedcb5b5bb59a3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=5754, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaf031989224e839925053ffcf0584a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=5754, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60efca4b0e754cfe82a5add7dcb1c6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=5754, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b39c94dd82c4ac897e09947dddb26b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=5754, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9348957641c64b1799cf4d48d2ee2dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=5754, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3d80eaf45e4b968a3645cce9c186d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=5754, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:55: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:56: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4027, 70)\n",
      "(1727, 70)\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# load and preprocess the datasets\n",
    "#############################################\n",
    "target = 'user_rating_ver'\n",
    "def load_and_prec():\n",
    "    \n",
    "    DATA_PATH = \"../input\"\n",
    "\n",
    "    ## Data Read and Join\n",
    "    data = pd.read_csv(os.path.join(DATA_PATH,\"app-store-apple-data-set-10k-apps/AppleStore.csv\"))\n",
    "    description_data = pd.read_csv(os.path.join(DATA_PATH,\"app-store-apple-data-set-10k-apps/appleStore_description.csv\"))\n",
    "    # Join the Data\n",
    "    data = data.set_index(\"id\")\n",
    "    description_data = description_data.set_index(\"id\")\n",
    "    full_data = data.join(description_data,lsuffix='_left', rsuffix='')\n",
    "    full_data.loc[:,\"size_bytes\"] = full_data[\"size_bytes_left\"]\n",
    "    del full_data[\"size_bytes_left\"],full_data[\"track_name_left\"],full_data[\"Unnamed: 0\"]\n",
    "    # select the user rating and the app description    \n",
    "    # new_data = full_data[['app_desc', target]]\n",
    "    catagorical = 'prime_genre'\n",
    "    full_data = full_data.loc[full_data[target] != 0]\n",
    "    total_features = [catagorical, 'size_bytes', 'price', 'rating_count_tot', 'rating_count_ver', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', target, 'app_desc']\n",
    "#     new_data = full_data[['app_desc', target]]\n",
    "    new_data = full_data.loc[:,total_features]\n",
    "    temp_data = new_data.loc[:, catagorical]\n",
    "    temp_data = pd.get_dummies(new_data.loc[:, catagorical])\n",
    "    new_data = new_data.join(temp_data)\n",
    "    # lower\n",
    "    new_data.loc[:,'app_desc'] = new_data['app_desc'].progress_apply(lambda x: x.lower())\n",
    "    # clean the text\n",
    "    new_data.loc[:,'app_desc'] = new_data['app_desc'].progress_apply(lambda x: clean_text(x))\n",
    "    # clean numbers\n",
    "    new_data.loc[:,'app_desc'] = new_data['app_desc'].progress_apply(lambda x: clean_numbers(x))\n",
    "    # clean speelings\n",
    "    new_data.loc[:,'app_desc'] = new_data['app_desc'].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    # fill up the missing values\n",
    "    new_data.loc[:,'app_desc'] = new_data['app_desc'].fillna(\"_##_\").values\n",
    "    \n",
    "    ###################### Add Features ###############################\n",
    "    #  https://github.com/wongchunghang/toxic-comment-challenge-lstm/blob/master/toxic_comment_9872_model.ipynb\n",
    "    new_data, feature_list = add_features(new_data)\n",
    "    y = new_data[[target]]\n",
    "    cols = [i for i in new_data.columns if i not in [target]]\n",
    "    X = new_data[cols]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#     print(X_train.columns)\n",
    "    feature_cols = [i for i in new_data.columns if i not in [catagorical, target, 'app_desc']]\n",
    "    features = X_train.loc[:,feature_cols].fillna(0)\n",
    "    test_features = X_test.loc[:,feature_cols].fillna(0)\n",
    "    \n",
    "    feature_list = feature_list + feature_cols\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((features, test_features)))\n",
    "    features = ss.transform(features)\n",
    "    test_features = ss.transform(test_features)\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    X_train = X_train['app_desc'].fillna(\"_##_\").values\n",
    "    X_test = X_test['app_desc'].fillna(\"_##_\").values\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X_train))\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    y_train = y_train[target].values\n",
    "    ## Pad the sentences \n",
    "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(SEED)\n",
    "    trn_idx = np.random.permutation(len(X_train))\n",
    "\n",
    "    X_train = X_train[trn_idx]\n",
    "    y_train = y_train[trn_idx]\n",
    "    features = features[trn_idx]\n",
    "    \n",
    "    return full_data, X_train, X_test, y_train, y_test, features, tokenizer.word_index, test_features, feature_list\n",
    "\n",
    "full_data, X_train, X_test, y_train, y_test, features, word_index, test_features, feature_list = load_and_prec()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "cnt = full_data[target].value_counts().reindex(np.arange(0.0, 5.5, 0.5)).plot(kind='bar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'        \n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "        \n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word not in word_index:\n",
    "                continue\n",
    "            i = word_index[word]\n",
    "            if i >= max_features:\n",
    "                continue\n",
    "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "            if len(embedding_vector) == 300:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'   \n",
    "    emb_mean, emb_std = -0.0033469985, 0.109855495\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "        \n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word not in word_index:\n",
    "                continue\n",
    "            i = word_index[word]\n",
    "            if i >= max_features:\n",
    "                continue\n",
    "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "            if len(embedding_vector) == 300:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    emb_mean, emb_std = -0.0053247833,0.49346462\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "        \n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"latin-1\") as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word not in word_index:\n",
    "                continue\n",
    "            i = word_index[word]\n",
    "            if i >= max_features:\n",
    "                continue\n",
    "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "            if len(embedding_vector) == 300:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "\n",
    "glove_embeddings = load_glove(word_index)\n",
    "paragram_embeddings = load_para(word_index)\n",
    "fasttext_embeddings = load_fasttext(word_index)\n",
    "\n",
    "embedding_matrix = np.mean([glove_embeddings, paragram_embeddings, fasttext_embeddings], axis=0)\n",
    "#np.mean([glove_embeddings, paragram_embeddings, fasttext_embeddings], axis=0)\n",
    "#embedding_matrix = np.concatenate([glove_embeddings, paragram_embeddings], axis=0)\n",
    "\n",
    "# add_lower(embedding_matrix, vocab)\n",
    "del glove_embeddings, paragram_embeddings, fasttext_embeddings\n",
    "gc.collect()\n",
    "\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Recurrence Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_dim = 300\n",
    "use_pretrained_embedding = True\n",
    "\n",
    "hidden_size = 400\n",
    "gru_len = hidden_size\n",
    "\n",
    "Routings = 4 #5\n",
    "Num_capsule = 5\n",
    "Dim_capsule = 5 #16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "LR = 0.0001\n",
    "T_epsilon = 1e-7\n",
    "num_classes = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Initialization\n",
    "Build the Class of the CyclicLR, Embed_Layer, GRU_Layer, Caps_Layer, Attention Layer, Gaussian noise regularizer and the NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs\n",
    "        \n",
    "class Embed_Layer(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None, embedding_dim=300):\n",
    "        super(Embed_Layer, self).__init__()\n",
    "        self.encoder = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        if use_pretrained_embedding:\n",
    "            # self.encoder.weight.data.copy_(t.from_numpy(np.load(embedding_path))) # 方法一，加载np.save的npy文件\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(embedding_matrix))  # 方法二\n",
    "\n",
    "    def forward(self, x, dropout_p=0.25):\n",
    "        return nn.Dropout(p=dropout_p)(self.encoder(x))\n",
    "\n",
    "\n",
    "class GRU_Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU_Layer, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=300,\n",
    "                          hidden_size=gru_len,\n",
    "                          bidirectional=True)\n",
    "        # # if you uncomment /*from rnn_revised import * */, uncomment following code aswell\n",
    "        # self.gru = RNNHardSigmoid('GRU', input_size=300,\n",
    "        #                           hidden_size=gru_len,\n",
    "        #                           bidirectional=True)\n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gru(x)\n",
    "\n",
    "\n",
    "# core caps_layer with squash func\n",
    "class Caps_Layer(nn.Module):\n",
    "    def __init__(self, input_dim_capsule=gru_len * 2, num_capsule=Num_capsule, dim_capsule=Dim_capsule, \\\n",
    "                 routings=Routings, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Caps_Layer, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = self.squash\n",
    "        else:\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        if self.share_weights:\n",
    "            self.W = nn.Parameter(\n",
    "                nn.init.xavier_normal_(t.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
    "        else:\n",
    "            self.W = nn.Parameter(\n",
    "                t.randn(BATCH_SIZE, input_dim_capsule, self.num_capsule * self.dim_capsule))  # 64即batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = t.matmul(x, self.W)\n",
    "        else:\n",
    "            print('add later')\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        input_num_capsule = x.size(1)\n",
    "        u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "                                      self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3)  # 转成(batch_size,num_capsule,input_num_capsule,dim_capsule)\n",
    "        b = t.zeros_like(u_hat_vecs[:, :, :, 0])  # (batch_size,num_capsule,input_num_capsule)\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            b = b.permute(0, 2, 1)\n",
    "            c = F.softmax(b, dim=2)\n",
    "            c = c.permute(0, 2, 1)\n",
    "            b = b.permute(0, 2, 1)\n",
    "            outputs = self.activation(t.einsum('bij,bijk->bik', (c, u_hat_vecs)))  # batch matrix multiplication\n",
    "            # outputs shape (batch_size, num_capsule, dim_capsule)\n",
    "            if i < self.routings - 1:\n",
    "                b = t.einsum('bik,bijk->bij', (outputs, u_hat_vecs))  # batch matrix multiplication\n",
    "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "    # text version of squash, slight different from original one\n",
    "    def squash(self, x, axis=-1):\n",
    "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "        scale = t.sqrt(s_squared_norm + T_epsilon)\n",
    "        return x / scale\n",
    "    \n",
    "class Capsule_Main(nn.Module):\n",
    "    def __init__(self, embedding_matrix=None, vocab_size=None):\n",
    "        super(Capsule_Main, self).__init__()\n",
    "        self.embed_layer = Embed_Layer(embedding_matrix, vocab_size)\n",
    "        self.gru_layer = GRU_Layer()\n",
    "        self.gru_layer.init_weights()\n",
    "        self.caps_layer = Caps_Layer()\n",
    "        self.dense_layer = Dense_Layer()\n",
    "\n",
    "    def forward(self, content):\n",
    "        content1 = self.embed_layer(content)\n",
    "        content2, _ = self.gru_layer(content1)\n",
    "        content3 = self.caps_layer(content2)\n",
    "        output = self.dense_layer(content3)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"Gaussian noise regularizer.\n",
    "\n",
    "    Args:\n",
    "        sigma (float, optional): relative standard deviation used to generate the\n",
    "            noise. Relative means that it will be multiplied by the magnitude of\n",
    "            the value your are adding the noise to. This means that sigma can be\n",
    "            the same regardless of the scale of the vector.\n",
    "        is_relative_detach (bool, optional): whether to detach the variable before\n",
    "            computing the scale of the noise. If `False` then the scale of the noise\n",
    "            won't be seen as a constant but something to optimize: this will bias the\n",
    "            network to generate vectors with smaller values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma=0.1, is_relative_detach=True):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.is_relative_detach = is_relative_detach\n",
    "        self.noise = torch.tensor(0).cuda().float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.sigma != 0:\n",
    "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
    "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
    "            x = x + sampled_noise\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, mode=0, noise_flag=False):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        fc_layer = 16\n",
    "        fc_layer1 = 16\n",
    "\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru0 = nn.GRU(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.bn = nn.BatchNorm1d(16, momentum=0.5)\n",
    "#         self.linear = nn.Linear(hidden_size*8+1+len(feature_list), fc_layer1)\n",
    "        self.linear = nn.Linear(hidden_size*12+len(feature_list)-6, fc_layer1)#643:80 - 483:60 - 323:40\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(fc_layer**2,fc_layer)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.out = nn.Linear(fc_layer, 1)\n",
    "        self.lincaps = nn.Linear(Num_capsule * Dim_capsule, 1)\n",
    "        self.caps_layer = Caps_Layer()\n",
    "        self.noise = GaussianNoise()\n",
    "        #### 0 : lstm + gru, 1 : gru + gru, 2 : lstm + lstm\n",
    "        self.mode = mode\n",
    "        self.noise_flag = noise_flag\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x[0])\n",
    "        h_embedding = torch.squeeze(\n",
    "            self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        if self.noise_flag:  \n",
    "            h_embedding = self.noise(h_embedding)\n",
    "        \n",
    "        if self.mode == 0: \n",
    "            h_lstm, _ = self.lstm(h_embedding)\n",
    "            h_gru, _ = self.gru(h_lstm)\n",
    "        elif self.mode == 1:\n",
    "            h_gru_0, _ = self.gru0(h_embedding)\n",
    "            h_gru, _ = self.gru(h_gru_0)\n",
    "        else:\n",
    "            h_lstm, _ = self.lstm(h_embedding)\n",
    "            h_lstm_2, _ = self.lstm2(h_lstm)\n",
    "\n",
    "\n",
    "        \n",
    "        ##Capsule Layer      \n",
    "        if self.mode != 2:\n",
    "            content3 = self.caps_layer(h_gru)\n",
    "        else:\n",
    "            content3 = self.caps_layer(h_lstm_2)\n",
    "        \n",
    "        content3 = self.dropout(content3)\n",
    "        batch_size = content3.size(0)\n",
    "        content3 = content3.view(batch_size, -1)\n",
    "        content3 = self.relu(self.lincaps(content3))\n",
    "\n",
    "        ##Attention Layer\n",
    "        if self.mode == 0:\n",
    "            h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "            h_gru_atten = self.gru_attention(h_gru)\n",
    "        elif self.mode == 1:\n",
    "            h_gru_0_atten = self.gru_attention(h_gru_0)\n",
    "            h_gru_atten = self.gru_attention(h_gru)\n",
    "        else:\n",
    "            h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "            h_lstm_atten_2 = self.lstm_attention(h_lstm_2)\n",
    "        \n",
    "        if self.mode == 0:\n",
    "            avg_pool = torch.mean(h_gru, 1)\n",
    "            max_pool, _ = torch.max(h_gru, 1)\n",
    "            avg_pool_0 = torch.mean(h_lstm, 1)\n",
    "            max_pool_0, _ = torch.max(h_lstm, 1)\n",
    "        elif self.mode == 1:\n",
    "            avg_pool = torch.mean(h_gru, 1)\n",
    "            max_pool, _ = torch.max(h_gru, 1)\n",
    "            avg_pool_0 = torch.mean(h_gru_0, 1)\n",
    "            max_pool_0, _ = torch.max(h_gru_0, 1)\n",
    "        else:\n",
    "            avg_pool = torch.mean(h_lstm_2, 1)\n",
    "            max_pool, _ = torch.max(h_lstm_2, 1)\n",
    "            avg_pool_0 = torch.mean(h_lstm, 1)\n",
    "            max_pool_0, _ = torch.max(h_lstm, 1)\n",
    "            \n",
    "        f = torch.tensor(x[1], dtype=torch.float).cuda()\n",
    "        \n",
    "        if self.mode == 0:\n",
    "            conc = torch.cat((h_lstm_atten, h_gru_atten, content3, avg_pool_0, max_pool_0, avg_pool, max_pool, f), 1)\n",
    "        elif self.mode == 1:\n",
    "            conc = torch.cat((h_gru_0_atten, h_gru_atten, content3, avg_pool_0, max_pool_0, avg_pool, max_pool, f), 1)\n",
    "        else:\n",
    "            conc = torch.cat((h_lstm_atten, h_lstm_atten_2, content3, avg_pool_0, max_pool_0, avg_pool, max_pool, f), 1)\n",
    "        \n",
    "        conc = self.linear(conc)\n",
    "        if self.noise_flag:\n",
    "            conc = self.noise(conc)\n",
    "        conc = self.relu(conc)\n",
    "        conc = self.bn(conc)\n",
    "        conc = self.dropout(conc)\n",
    "\n",
    "        out = self.out(conc)\n",
    "        out = self.sigmoid(out)*5\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "#         check = x - y\n",
    "#         if abs(x-y) > 0.5:\n",
    "#         return t.mean(t.exp(2*t.abs(x-y))) + t.mean(t.pow((x - y), 2))\n",
    "        return t.mean(t.pow((x - y), 2))\n",
    "#         return t.mean(t.pow((x - y), 2))\n",
    "#         return t.mean(t.exp((x - y) * 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/40 \t loss=3.0449 \t val_loss=2.6479 \t time=1.34s\n",
      "Epoch 2/40 \t loss=2.9183 \t val_loss=3.0170 \t time=1.26s\n",
      "Epoch 3/40 \t loss=2.8532 \t val_loss=1.9996 \t time=1.26s\n",
      "Epoch 4/40 \t loss=2.7473 \t val_loss=2.8033 \t time=1.26s\n",
      "Epoch 5/40 \t loss=2.6548 \t val_loss=1.2024 \t time=1.26s\n",
      "Epoch 6/40 \t loss=2.5525 \t val_loss=2.4745 \t time=1.26s\n",
      "Epoch 7/40 \t loss=2.4467 \t val_loss=2.4306 \t time=1.26s\n",
      "Epoch 8/40 \t loss=2.3264 \t val_loss=3.7817 \t time=1.26s\n",
      "Epoch 9/40 \t loss=2.1938 \t val_loss=1.4386 \t time=1.26s\n",
      "Epoch 10/40 \t loss=2.0764 \t val_loss=2.2976 \t time=1.26s\n",
      "Epoch 11/40 \t loss=2.0630 \t val_loss=1.0444 \t time=1.27s\n",
      "Epoch 12/40 \t loss=1.9906 \t val_loss=1.7393 \t time=1.26s\n",
      "Epoch 13/40 \t loss=2.2775 \t val_loss=2.6736 \t time=1.26s\n",
      "Epoch 14/40 \t loss=2.3970 \t val_loss=2.3627 \t time=1.26s\n",
      "Epoch 15/40 \t loss=2.2751 \t val_loss=2.2770 \t time=1.27s\n",
      "Epoch 16/40 \t loss=2.1191 \t val_loss=1.8431 \t time=1.27s\n",
      "Epoch 17/40 \t loss=1.6856 \t val_loss=2.4523 \t time=1.26s\n",
      "Epoch 18/40 \t loss=1.4206 \t val_loss=1.5204 \t time=1.26s\n",
      "Epoch 19/40 \t loss=1.2967 \t val_loss=1.3460 \t time=1.26s\n",
      "Epoch 20/40 \t loss=1.1958 \t val_loss=1.5875 \t time=1.27s\n",
      "Epoch 21/40 \t loss=1.0943 \t val_loss=1.1327 \t time=1.26s\n",
      "Epoch 22/40 \t loss=1.0326 \t val_loss=0.9156 \t time=1.26s\n",
      "Epoch 23/40 \t loss=0.9711 \t val_loss=0.8947 \t time=1.26s\n",
      "Epoch 24/40 \t loss=0.8923 \t val_loss=0.9146 \t time=1.27s\n",
      "Epoch 25/40 \t loss=0.8740 \t val_loss=0.8319 \t time=1.26s\n",
      "Epoch 26/40 \t loss=0.8340 \t val_loss=0.7868 \t time=1.26s\n",
      "Epoch 27/40 \t loss=0.8146 \t val_loss=0.8598 \t time=1.26s\n",
      "Epoch 28/40 \t loss=0.8099 \t val_loss=0.8426 \t time=1.27s\n",
      "Epoch 29/40 \t loss=0.7685 \t val_loss=0.8434 \t time=1.26s\n",
      "Epoch 30/40 \t loss=0.7772 \t val_loss=0.7972 \t time=1.26s\n",
      "Epoch 31/40 \t loss=0.7535 \t val_loss=0.8574 \t time=1.44s\n",
      "Epoch 32/40 \t loss=0.7349 \t val_loss=0.7478 \t time=1.26s\n",
      "Epoch 33/40 \t loss=0.7389 \t val_loss=0.8060 \t time=1.27s\n",
      "Epoch 34/40 \t loss=0.7296 \t val_loss=0.7605 \t time=1.27s\n",
      "Epoch 35/40 \t loss=0.7121 \t val_loss=0.7892 \t time=1.26s\n",
      "Epoch 36/40 \t loss=0.6868 \t val_loss=0.6565 \t time=1.27s\n",
      "Epoch 37/40 \t loss=0.6843 \t val_loss=0.7420 \t time=1.27s\n",
      "Epoch 38/40 \t loss=0.6362 \t val_loss=0.8100 \t time=1.28s\n",
      "Epoch 39/40 \t loss=0.6214 \t val_loss=0.9149 \t time=1.27s\n",
      "Epoch 40/40 \t loss=0.6591 \t val_loss=0.7290 \t time=1.27s\n",
      "Fold 2\n",
      "Epoch 1/40 \t loss=4.0319 \t val_loss=4.6660 \t time=1.27s\n",
      "Epoch 2/40 \t loss=3.8539 \t val_loss=4.2421 \t time=1.27s\n",
      "Epoch 3/40 \t loss=3.7580 \t val_loss=3.9907 \t time=1.28s\n",
      "Epoch 4/40 \t loss=3.6714 \t val_loss=3.5384 \t time=1.27s\n",
      "Epoch 5/40 \t loss=3.5402 \t val_loss=2.8432 \t time=1.27s\n",
      "Epoch 6/40 \t loss=3.3991 \t val_loss=2.9772 \t time=1.27s\n",
      "Epoch 7/40 \t loss=3.2838 \t val_loss=2.2538 \t time=1.27s\n",
      "Epoch 8/40 \t loss=3.1139 \t val_loss=4.7260 \t time=1.28s\n",
      "Epoch 9/40 \t loss=2.9592 \t val_loss=1.8382 \t time=1.27s\n",
      "Epoch 10/40 \t loss=2.7690 \t val_loss=2.4125 \t time=1.28s\n",
      "Epoch 11/40 \t loss=2.5835 \t val_loss=1.5674 \t time=1.29s\n",
      "Epoch 12/40 \t loss=2.4131 \t val_loss=1.4539 \t time=1.28s\n",
      "Epoch 13/40 \t loss=2.1936 \t val_loss=2.6978 \t time=1.28s\n",
      "Epoch 14/40 \t loss=2.0086 \t val_loss=1.6798 \t time=1.28s\n",
      "Epoch 15/40 \t loss=1.7772 \t val_loss=2.2445 \t time=1.28s\n",
      "Epoch 16/40 \t loss=1.5440 \t val_loss=1.1574 \t time=1.28s\n",
      "Epoch 17/40 \t loss=1.3333 \t val_loss=2.2445 \t time=1.27s\n",
      "Epoch 18/40 \t loss=1.1422 \t val_loss=2.8252 \t time=1.27s\n",
      "Epoch 19/40 \t loss=0.9699 \t val_loss=1.3951 \t time=1.28s\n",
      "Epoch 20/40 \t loss=0.7714 \t val_loss=1.4482 \t time=1.28s\n",
      "Epoch 21/40 \t loss=0.6420 \t val_loss=0.8153 \t time=1.28s\n",
      "Epoch 22/40 \t loss=0.5511 \t val_loss=1.1405 \t time=1.28s\n",
      "Epoch 23/40 \t loss=0.4698 \t val_loss=2.5357 \t time=1.28s\n",
      "Epoch 24/40 \t loss=0.3844 \t val_loss=1.0325 \t time=1.28s\n",
      "Epoch 25/40 \t loss=0.3594 \t val_loss=0.8006 \t time=1.28s\n",
      "Epoch 26/40 \t loss=0.3062 \t val_loss=0.9119 \t time=1.28s\n",
      "Epoch 27/40 \t loss=0.2926 \t val_loss=0.8013 \t time=1.28s\n",
      "Epoch 28/40 \t loss=0.2600 \t val_loss=0.7891 \t time=1.28s\n",
      "Epoch 29/40 \t loss=0.2367 \t val_loss=0.7473 \t time=1.28s\n",
      "Epoch 30/40 \t loss=0.2339 \t val_loss=0.6844 \t time=1.28s\n",
      "Epoch 31/40 \t loss=0.2248 \t val_loss=0.7148 \t time=1.28s\n",
      "Epoch 32/40 \t loss=0.2046 \t val_loss=0.6860 \t time=1.28s\n",
      "Epoch 33/40 \t loss=0.1994 \t val_loss=0.7718 \t time=1.28s\n",
      "Epoch 34/40 \t loss=0.1964 \t val_loss=0.8194 \t time=1.28s\n",
      "Epoch 35/40 \t loss=0.2052 \t val_loss=0.8248 \t time=1.28s\n",
      "Epoch 36/40 \t loss=0.1861 \t val_loss=0.7527 \t time=1.28s\n",
      "Epoch 37/40 \t loss=0.1952 \t val_loss=0.7903 \t time=1.28s\n",
      "Epoch 38/40 \t loss=0.1940 \t val_loss=0.7377 \t time=1.28s\n",
      "Epoch 39/40 \t loss=0.1907 \t val_loss=0.7860 \t time=1.28s\n",
      "Epoch 40/40 \t loss=0.1904 \t val_loss=0.8113 \t time=1.28s\n",
      "Fold 3\n",
      "Epoch 1/40 \t loss=3.8809 \t val_loss=4.0031 \t time=1.28s\n",
      "Epoch 2/40 \t loss=3.6693 \t val_loss=3.0027 \t time=1.28s\n",
      "Epoch 3/40 \t loss=3.5646 \t val_loss=3.0476 \t time=1.28s\n",
      "Epoch 4/40 \t loss=3.4353 \t val_loss=2.6015 \t time=1.28s\n",
      "Epoch 5/40 \t loss=3.3152 \t val_loss=3.1773 \t time=1.28s\n",
      "Epoch 6/40 \t loss=3.2019 \t val_loss=2.4051 \t time=1.28s\n",
      "Epoch 7/40 \t loss=3.0630 \t val_loss=3.9032 \t time=1.28s\n",
      "Epoch 8/40 \t loss=2.9400 \t val_loss=3.6507 \t time=1.46s\n",
      "Epoch 9/40 \t loss=2.7529 \t val_loss=2.3210 \t time=1.28s\n",
      "Epoch 10/40 \t loss=2.5878 \t val_loss=2.9486 \t time=1.28s\n",
      "Epoch 11/40 \t loss=2.3948 \t val_loss=0.9238 \t time=1.28s\n",
      "Epoch 12/40 \t loss=2.2713 \t val_loss=0.8271 \t time=1.28s\n",
      "Epoch 13/40 \t loss=2.1707 \t val_loss=4.3292 \t time=1.28s\n",
      "Epoch 14/40 \t loss=1.9527 \t val_loss=0.8773 \t time=1.28s\n",
      "Epoch 15/40 \t loss=1.8135 \t val_loss=1.7674 \t time=1.28s\n",
      "Epoch 16/40 \t loss=1.6029 \t val_loss=1.7804 \t time=1.28s\n",
      "Epoch 17/40 \t loss=1.4198 \t val_loss=0.8959 \t time=1.28s\n",
      "Epoch 18/40 \t loss=1.2631 \t val_loss=1.0614 \t time=1.29s\n",
      "Epoch 19/40 \t loss=1.0819 \t val_loss=0.9548 \t time=1.28s\n",
      "Epoch 20/40 \t loss=0.9633 \t val_loss=0.9812 \t time=1.28s\n",
      "Epoch 21/40 \t loss=0.8229 \t val_loss=0.7682 \t time=1.29s\n",
      "Epoch 22/40 \t loss=0.6935 \t val_loss=0.9124 \t time=1.28s\n",
      "Epoch 23/40 \t loss=0.6718 \t val_loss=1.0461 \t time=1.28s\n",
      "Epoch 24/40 \t loss=0.5777 \t val_loss=0.8525 \t time=1.28s\n",
      "Epoch 25/40 \t loss=0.4871 \t val_loss=0.7519 \t time=1.29s\n",
      "Epoch 26/40 \t loss=0.4248 \t val_loss=0.7167 \t time=1.29s\n",
      "Epoch 27/40 \t loss=0.3679 \t val_loss=0.7832 \t time=1.28s\n",
      "Epoch 28/40 \t loss=0.3093 \t val_loss=0.9009 \t time=1.28s\n",
      "Epoch 29/40 \t loss=0.2946 \t val_loss=0.7924 \t time=1.28s\n",
      "Epoch 30/40 \t loss=0.2587 \t val_loss=0.9771 \t time=1.28s\n",
      "Epoch 31/40 \t loss=0.2389 \t val_loss=0.8543 \t time=1.28s\n",
      "Epoch 32/40 \t loss=0.2207 \t val_loss=0.8390 \t time=1.28s\n",
      "Epoch 33/40 \t loss=0.2398 \t val_loss=0.9347 \t time=1.28s\n",
      "Epoch 34/40 \t loss=0.2004 \t val_loss=0.9139 \t time=1.28s\n",
      "Epoch 35/40 \t loss=0.2084 \t val_loss=0.8644 \t time=1.28s\n",
      "Epoch 36/40 \t loss=0.1891 \t val_loss=0.7943 \t time=1.28s\n",
      "Epoch 37/40 \t loss=0.1953 \t val_loss=0.8418 \t time=1.28s\n",
      "Epoch 38/40 \t loss=0.1691 \t val_loss=1.0733 \t time=1.28s\n",
      "Epoch 39/40 \t loss=0.1859 \t val_loss=0.8155 \t time=1.28s\n",
      "Epoch 40/40 \t loss=0.1687 \t val_loss=0.8298 \t time=1.28s\n",
      "Fold 4\n",
      "Epoch 1/40 \t loss=3.9943 \t val_loss=4.3745 \t time=1.27s\n",
      "Epoch 2/40 \t loss=3.7994 \t val_loss=3.0964 \t time=1.28s\n",
      "Epoch 3/40 \t loss=3.7186 \t val_loss=2.4849 \t time=1.28s\n",
      "Epoch 4/40 \t loss=3.5944 \t val_loss=7.0333 \t time=1.28s\n",
      "Epoch 5/40 \t loss=3.4951 \t val_loss=3.1096 \t time=1.28s\n",
      "Epoch 6/40 \t loss=3.3905 \t val_loss=3.7433 \t time=1.28s\n",
      "Epoch 7/40 \t loss=3.2219 \t val_loss=3.5889 \t time=1.28s\n",
      "Epoch 8/40 \t loss=3.0781 \t val_loss=3.3410 \t time=1.28s\n",
      "Epoch 9/40 \t loss=2.9009 \t val_loss=3.4835 \t time=1.28s\n",
      "Epoch 10/40 \t loss=2.6986 \t val_loss=2.9649 \t time=1.28s\n",
      "Epoch 11/40 \t loss=2.4895 \t val_loss=4.6270 \t time=1.29s\n",
      "Epoch 12/40 \t loss=2.2708 \t val_loss=4.0332 \t time=1.29s\n",
      "Epoch 13/40 \t loss=2.0333 \t val_loss=2.0427 \t time=1.29s\n",
      "Epoch 14/40 \t loss=1.7455 \t val_loss=1.8602 \t time=1.29s\n",
      "Epoch 15/40 \t loss=1.5060 \t val_loss=3.9351 \t time=1.29s\n",
      "Epoch 16/40 \t loss=1.3227 \t val_loss=3.0881 \t time=1.30s\n",
      "Epoch 17/40 \t loss=1.1364 \t val_loss=1.5112 \t time=1.29s\n",
      "Epoch 18/40 \t loss=0.9630 \t val_loss=1.3814 \t time=1.28s\n",
      "Epoch 19/40 \t loss=0.8103 \t val_loss=0.7751 \t time=1.28s\n",
      "Epoch 20/40 \t loss=0.6908 \t val_loss=0.6920 \t time=1.28s\n",
      "Epoch 21/40 \t loss=0.5906 \t val_loss=0.7582 \t time=1.28s\n",
      "Epoch 22/40 \t loss=0.5086 \t val_loss=0.9238 \t time=1.28s\n",
      "Epoch 23/40 \t loss=0.4269 \t val_loss=1.2346 \t time=1.28s\n",
      "Epoch 24/40 \t loss=0.3654 \t val_loss=1.0098 \t time=1.28s\n",
      "Epoch 25/40 \t loss=0.3584 \t val_loss=0.7185 \t time=1.28s\n",
      "Epoch 26/40 \t loss=0.3051 \t val_loss=0.6396 \t time=1.28s\n",
      "Epoch 27/40 \t loss=0.2914 \t val_loss=1.1708 \t time=1.28s\n",
      "Epoch 28/40 \t loss=0.2879 \t val_loss=0.6804 \t time=1.28s\n",
      "Epoch 29/40 \t loss=0.2433 \t val_loss=0.7582 \t time=1.29s\n",
      "Epoch 30/40 \t loss=0.2352 \t val_loss=0.7185 \t time=1.27s\n",
      "Epoch 31/40 \t loss=0.2230 \t val_loss=0.9995 \t time=1.28s\n",
      "Epoch 32/40 \t loss=0.2016 \t val_loss=0.7217 \t time=1.46s\n",
      "Epoch 33/40 \t loss=0.2040 \t val_loss=0.8022 \t time=1.29s\n",
      "Epoch 34/40 \t loss=0.1820 \t val_loss=0.6622 \t time=1.28s\n",
      "Epoch 35/40 \t loss=0.1801 \t val_loss=0.7203 \t time=1.28s\n",
      "Epoch 36/40 \t loss=0.1691 \t val_loss=0.7308 \t time=1.28s\n",
      "Epoch 37/40 \t loss=0.1793 \t val_loss=0.6585 \t time=1.28s\n",
      "Epoch 38/40 \t loss=0.1736 \t val_loss=0.7924 \t time=1.28s\n",
      "Epoch 39/40 \t loss=0.1742 \t val_loss=0.7897 \t time=1.28s\n",
      "Epoch 40/40 \t loss=0.1660 \t val_loss=0.7254 \t time=1.28s\n",
      "Fold 5\n",
      "Epoch 1/40 \t loss=4.3025 \t val_loss=3.9998 \t time=1.28s\n",
      "Epoch 2/40 \t loss=4.1614 \t val_loss=3.7150 \t time=1.28s\n",
      "Epoch 3/40 \t loss=4.0230 \t val_loss=3.5659 \t time=1.28s\n",
      "Epoch 4/40 \t loss=3.9173 \t val_loss=3.2882 \t time=1.28s\n",
      "Epoch 5/40 \t loss=3.7849 \t val_loss=3.5181 \t time=1.28s\n",
      "Epoch 6/40 \t loss=3.6575 \t val_loss=3.3598 \t time=1.28s\n",
      "Epoch 7/40 \t loss=3.5034 \t val_loss=4.1007 \t time=1.28s\n",
      "Epoch 8/40 \t loss=3.3214 \t val_loss=3.1044 \t time=1.29s\n",
      "Epoch 9/40 \t loss=3.0781 \t val_loss=1.9720 \t time=1.28s\n",
      "Epoch 10/40 \t loss=2.8488 \t val_loss=2.8986 \t time=1.28s\n",
      "Epoch 11/40 \t loss=2.5701 \t val_loss=3.3057 \t time=1.28s\n",
      "Epoch 12/40 \t loss=2.2761 \t val_loss=1.7873 \t time=1.28s\n",
      "Epoch 13/40 \t loss=2.0347 \t val_loss=1.9965 \t time=1.28s\n",
      "Epoch 14/40 \t loss=1.7852 \t val_loss=1.6992 \t time=1.28s\n",
      "Epoch 15/40 \t loss=1.5480 \t val_loss=2.7765 \t time=1.28s\n",
      "Epoch 16/40 \t loss=1.3089 \t val_loss=1.2993 \t time=1.28s\n",
      "Epoch 17/40 \t loss=1.1437 \t val_loss=1.2699 \t time=1.28s\n",
      "Epoch 18/40 \t loss=0.9660 \t val_loss=1.3470 \t time=1.28s\n",
      "Epoch 19/40 \t loss=0.8355 \t val_loss=0.9723 \t time=1.28s\n",
      "Epoch 20/40 \t loss=0.7224 \t val_loss=0.8728 \t time=1.28s\n",
      "Epoch 21/40 \t loss=0.6396 \t val_loss=0.7228 \t time=1.28s\n",
      "Epoch 22/40 \t loss=0.5343 \t val_loss=0.7333 \t time=1.28s\n",
      "Epoch 23/40 \t loss=0.4531 \t val_loss=0.9111 \t time=1.28s\n",
      "Epoch 24/40 \t loss=0.4044 \t val_loss=0.9396 \t time=1.28s\n",
      "Epoch 25/40 \t loss=0.3724 \t val_loss=0.7750 \t time=1.28s\n",
      "Epoch 26/40 \t loss=0.3356 \t val_loss=0.7815 \t time=1.27s\n",
      "Epoch 27/40 \t loss=0.2887 \t val_loss=0.7698 \t time=1.27s\n",
      "Epoch 28/40 \t loss=0.2662 \t val_loss=1.0046 \t time=1.28s\n",
      "Epoch 29/40 \t loss=0.2537 \t val_loss=0.7350 \t time=1.28s\n",
      "Epoch 30/40 \t loss=0.2424 \t val_loss=0.6342 \t time=1.28s\n",
      "Epoch 31/40 \t loss=0.2033 \t val_loss=0.7739 \t time=1.28s\n",
      "Epoch 32/40 \t loss=0.2391 \t val_loss=0.7929 \t time=1.28s\n",
      "Epoch 33/40 \t loss=0.1964 \t val_loss=0.7240 \t time=1.28s\n",
      "Epoch 34/40 \t loss=0.1973 \t val_loss=0.7663 \t time=1.28s\n",
      "Epoch 35/40 \t loss=0.2107 \t val_loss=0.8034 \t time=1.28s\n",
      "Epoch 36/40 \t loss=0.1722 \t val_loss=0.6904 \t time=1.28s\n",
      "Epoch 37/40 \t loss=0.1556 \t val_loss=0.7767 \t time=1.28s\n",
      "Epoch 38/40 \t loss=0.1866 \t val_loss=0.7262 \t time=1.28s\n",
      "Epoch 39/40 \t loss=0.1738 \t val_loss=0.7549 \t time=1.28s\n",
      "Epoch 40/40 \t loss=0.1624 \t val_loss=0.7227 \t time=1.28s\n",
      "All \t loss=0.2693 \t val_loss=0.7636 \t \n",
      "Mean_squared_error: 0.7753\n"
     ]
    }
   ],
   "source": [
    "# matrix for the out-of-fold predictions\n",
    "train_preds = np.zeros((len(X_train)))\n",
    "# matrix for the predictions on the test set\n",
    "test_preds = np.zeros((len(X_test)))\n",
    "\n",
    "# split the dataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(target_labels)\n",
    "# lb = preprocessing.LabelBinarizer()\n",
    "# y_train_t = lb.fit(y_train*2)\n",
    "# splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(X_train, y_train_t))\n",
    "\n",
    "splits = list(KFold(n_splits= n_splits, random_state=None, shuffle=False).split(X_train, y_train))\n",
    "\n",
    "x_test_cuda = torch.tensor(X_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size= batch_size, drop_last = False, shuffle= False)\n",
    "\n",
    "avg_losses_f = []\n",
    "avg_val_losses_f = []\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "    # split data in train / validation according to the KFold indeces\n",
    "    # also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    features = np.array(features)\n",
    "\n",
    "    x_train_fold = torch.tensor(X_train[train_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    kfold_X_features = features[train_idx.astype(int)]\n",
    "    kfold_X_valid_features = features[valid_idx.astype(int)]\n",
    "    \n",
    "    x_val_fold = torch.tensor(X_train[valid_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    model = NeuralNet(mode=0, noise_flag=True)\n",
    "\n",
    "    # make sure everything in the model is running on the GPU\n",
    "    model.cuda()\n",
    "    \n",
    "    seed_everything(SEED+i)\n",
    "\n",
    "    # note that the model returns logit to take advantage of the log-sum-exp trick \n",
    "    # for numerical stability in the lossMSE\n",
    "    loss_fn = My_loss()\n",
    "\n",
    "    step_size = 300\n",
    "    base_lr, max_lr = 0.001, 0.003   \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                             lr=max_lr)\n",
    "    \n",
    "    ################################################################################################\n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "               step_size=step_size, mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "    ###############################################################################################\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train = MyDataset(train)\n",
    "    valid = MyDataset(valid)\n",
    "\n",
    "    ##No need to shuffle the data again here. Shuffling happens when splitting for kfolds.\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, drop_last = True, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, drop_last = True, shuffle=True)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    for epoch in range(n_epochs):\n",
    "        # set train mode of the model. This enables operations which are only applied during training like dropout\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "\n",
    "        avg_loss = 0.  \n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            ################################################################################################            \n",
    "            f = kfold_X_features[index]\n",
    "            y_pred = model([x_batch,f])\n",
    "            ################################################################################################\n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            ################################################################################################\n",
    "            # Compute and print loss.\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights\n",
    "            # of the model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n",
    "        model.eval()\n",
    "        \n",
    "        # predict all the samples in y_val_fold batch per batch\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(X_test)))\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "            f = kfold_X_valid_features[index]\n",
    "    \n",
    "            y_pred = model([x_batch,f]).detach()\n",
    "            \n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()[:, 0]\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "    avg_losses_f.append(avg_loss)\n",
    "    avg_val_losses_f.append(avg_val_loss) \n",
    "    # predict all samples in the test set batch per batch\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        f = test_features[i * batch_size:(i+1) * batch_size]\n",
    "        y_pred = model([x_batch,f]).detach()\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()[:, 0]\n",
    "        \n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)\n",
    "\n",
    "print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t '.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))\n",
    "\n",
    "def bestThresshold(y_train,train_preds):\n",
    "    print('Mean_squared_error: {:.4f}'.format(mean_squared_error(y_train, np.array(train_preds))))\n",
    "bestThresshold(y_test[target],test_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Prediction without 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test.columns\n",
    "# plt.figure(12)\n",
    "# cnt2 = y_test[target].value_counts().reindex(np.arange(0.0, 5.5, 0.5)).plot(kind='bar')\n",
    "# plt.figure(22)\n",
    "# cnt3 = y_test['adjust_prediction'].value_counts().reindex(np.arange(0.0, 5.5, 0.5)).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            user_rating_ver  prediction  adjust_prediction\n",
      "id                                                        \n",
      "640364616               4.5    4.420182                4.5\n",
      "1100292960              4.5    4.124547                4.0\n",
      "1074222293              3.5    4.306051                4.5\n",
      "1071976079              3.0    4.129136                4.0\n",
      "991558368               4.5    3.090697                3.0\n",
      "519052678               5.0    4.074309                4.0\n",
      "548946769               2.5    3.973880                4.0\n",
      "720825227               5.0    4.334541                4.5\n",
      "1088300283              5.0    4.236174                4.0\n",
      "448142450               4.5    3.364941                3.5\n",
      "341776037               4.5    3.717594                3.5\n",
      "1123795045              3.0    3.980859                4.0\n",
      "435476174               3.5    4.240653                4.0\n",
      "1078665338              2.0    3.466583                3.5\n",
      "1134867821              3.0    4.434385                4.5\n",
      "644106186               4.5    3.705821                3.5\n",
      "314819528               3.0    3.895263                4.0\n",
      "935216956               5.0    4.484059                4.5\n",
      "863960527               4.5    4.365762                4.5\n",
      "325658560               5.0    4.417080                4.5\n",
      "1040803415              5.0    4.384811                4.5\n",
      "1037420924              4.5    4.327079                4.5\n",
      "1051755724              5.0    3.647951                3.5\n",
      "1093260038              4.0    3.758010                4.0\n",
      "1149994032              3.5    4.127111                4.0\n",
      "421167112               4.0    4.101734                4.0\n",
      "724816878               4.5    4.283085                4.5\n",
      "908511009               4.5    4.235434                4.0\n",
      "1040120819              4.0    3.722423                3.5\n",
      "376510438               2.0    3.952711                4.0\n",
      "...                     ...         ...                ...\n",
      "1102362078              4.5    4.314011                4.5\n",
      "741597322               5.0    3.449076                3.5\n",
      "1106012225              4.0    4.264472                4.5\n",
      "1121685698              4.0    4.270562                4.5\n",
      "565951597               5.0    4.244194                4.0\n",
      "1041631196              5.0    4.430280                4.5\n",
      "1010801519              4.5    4.329959                4.5\n",
      "1119002115              2.0    4.260486                4.5\n",
      "1118353232              4.0    4.101054                4.0\n",
      "555610791               5.0    3.168718                3.0\n",
      "1108384425              5.0    3.962024                4.0\n",
      "1103310426              4.5    3.809518                4.0\n",
      "1000559499              4.5    4.250686                4.5\n",
      "907405413               4.0    4.244443                4.0\n",
      "1023146677              4.5    4.286470                4.5\n",
      "1054011814              3.5    3.935498                4.0\n",
      "372648912               3.5    3.969781                4.0\n",
      "967082741               4.5    4.322746                4.5\n",
      "529096824               4.5    4.427252                4.5\n",
      "1009134067              4.5    4.165115                4.0\n",
      "1055502792              4.0    4.318301                4.5\n",
      "1085652055              4.5    4.184370                4.0\n",
      "1080634585              4.0    4.106801                4.0\n",
      "1003138996              5.0    4.272495                4.5\n",
      "1168567265              5.0    4.373634                4.5\n",
      "331308914               4.5    4.425375                4.5\n",
      "1036723978              4.5    4.104440                4.0\n",
      "309025938               3.5    2.981813                3.0\n",
      "326979430               4.0    4.389020                4.5\n",
      "983868842               4.5    4.545297                4.5\n",
      "\n",
      "[1727 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3138390272148234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_adjust = []\n",
    "for i in range(0, len(test_preds)):\n",
    "    if test_preds[i] >= 0 and test_preds[i] < 1:\n",
    "        val = 1\n",
    "        test_adjust.append(val)\n",
    "    elif test_preds[i] >= 4.75 and test_preds[i] <= 5:\n",
    "        val = 5\n",
    "        test_adjust.append(val)\n",
    "    else:\n",
    "        k = 1\n",
    "        while k <= 10:\n",
    "            if test_preds[i] >= 0.5*k + 0.25 and test_preds[i] < 0.5*k + 0.75:\n",
    "                val = k*0.5+0.5\n",
    "                test_adjust.append(val)\n",
    "                break        \n",
    "            k += 1\n",
    "y_test['prediction'] = test_preds\n",
    "y_test['adjust_prediction'] = test_adjust\n",
    "print(y_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test[target].astype('S'), y_test['adjust_prediction'].astype('S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the analysis result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7f01339cb860>,\n",
       "  <matplotlib.lines.Line2D at 0x7f01339cbbe0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7f01339cbf28>,\n",
       "  <matplotlib.lines.Line2D at 0x7f01339d32b0>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7f01339cb710>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7f01339d35f8>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7f01339d3940>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "y_test.columns\n",
    "plt.figure(12)\n",
    "cnt2 = y_test[target].value_counts().reindex(np.arange(0.0, 5.5, 0.5)).plot(kind='bar')\n",
    "# plt.figure(22)\n",
    "# cnt3 = y_test['adjust_prediction'].value_counts().reindex(np.arange(0.0, 5.5, 0.5)).plot(kind='bar')\n",
    "plt.figure(21)\n",
    "order_test = sorted(y_test['prediction'].values)\n",
    "x = np.arange(len(order_test))+1\n",
    "plt.plot(x, order_test)\n",
    "plt.figure(22)\n",
    "plt.boxplot(order_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   5.,\n",
       "         11.,  28.,  58., 131., 266., 525., 630.,  71.,   1.,   0.]),\n",
       " array([0.  , 0.25, 0.5 , 0.75, 1.  , 1.25, 1.5 , 1.75, 2.  , 2.25, 2.5 ,\n",
       "        2.75, 3.  , 3.25, 3.5 , 3.75, 4.  , 4.25, 4.5 , 4.75, 5.  , 5.25]),\n",
       " <a list of 21 Patch objects>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADwlJREFUeJzt3X+MZWV9x/H3pyyowR8LMt1sdjddEjca0kQgE4rBmJaNBtC4+4cSTSsbssn8QxuMTSz2n8akf+g/oiQNyca1XVqrEtSwscS6WTDGpKCzsqKwWqZEsrsBdlRAKbEG/faPeWgH3HXOzNzLnXn2/Upu7nOe89x7vifAhyfPnHNuqgpJUr/+YNIFSJLGy6CXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdW7DpAsAuOiii2r79u2TLkOS1pUjR478tKqmlhq3JoJ++/btzM7OTroMSVpXkjw+ZJxLN5LUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1Lk1cWesJL1csrLPVY22jh44o5ekzhn0ktQ5g16SOmfQS1LnDHpJ6tygoE+yMcldSX6U5FiStyW5MMmhJI+29wva2CS5LclckoeSXD7eU5Ak/T5DZ/SfAb5eVW8B3gocA24BDlfVDuBw2wa4FtjRXjPA7SOtWJK0LEsGfZI3AO8A9gNU1a+r6hlgF3CgDTsA7G7tXcAdteB+YGOSzSOvXJI0yJAZ/cXAPPCPSR5M8tkk5wObquqJNuZJYFNrbwGOL/r8idYnSZqAIUG/AbgcuL2qLgP+m/9fpgGgqgpY1v1oSWaSzCaZnZ+fX85HJUnLMCToTwAnquqBtn0XC8H/1ItLMu39VNt/Eti26PNbW99LVNW+qpququmpqSV/xFyStEJLBn1VPQkcT/Lm1rUTeAQ4COxpfXuAu1v7IHBDu/rmSuDZRUs8kqRX2NCHmv0V8Pkk5wGPATey8D+JO5PsBR4Hrm9j7wGuA+aA59tYSdKEDAr6qjoKTJ9m187TjC3gplXWJUkaEe+MlaTOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjo39MfBJWnZkklXIHBGL0ndM+glqXMGvSR1zqCXpM4NCvokP0nygyRHk8y2vguTHEryaHu/oPUnyW1J5pI8lOTycZ6AJOn3W86M/s+q6tKqmm7btwCHq2oHcLhtA1wL7GivGeD2URUrSVq+1Szd7AIOtPYBYPei/jtqwf3AxiSbV3EcSdIqDA36Ar6R5EiSmda3qaqeaO0ngU2tvQU4vuizJ1rfSySZSTKbZHZ+fn4FpUuShhh6w9Tbq+pkkj8EDiX50eKdVVVJajkHrqp9wD6A6enpZX1WkjTcoBl9VZ1s76eArwJXAE+9uCTT3k+14SeBbYs+vrX1SZImYMmgT3J+kte92AbeBfwQOAjsacP2AHe39kHghnb1zZXAs4uWeCRJr7AhSzebgK9m4aEVG4B/raqvJ/kucGeSvcDjwPVt/D3AdcAc8Dxw48irliQNtmTQV9VjwFtP0/8zYOdp+gu4aSTVSZJWzTtjJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnRsc9EnOSfJgkq+17YuTPJBkLsmXkpzX+l/Vtufa/u3jKV2SNMRyZvQ3A8cWbX8SuLWq3gQ8Dext/XuBp1v/rW2cJGlCBgV9kq3Au4HPtu0AVwN3tSEHgN2tvatt0/bvbOMlSRMwdEb/aeCjwG/b9huBZ6rqhbZ9AtjS2luA4wBt/7Nt/EskmUkym2R2fn5+heVLkpayZNAneQ9wqqqOjPLAVbWvqqaranpqamqUXy1JWmTDgDFXAe9Nch3wauD1wGeAjUk2tFn7VuBkG38S2AacSLIBeAPws5FXLkkaZMkZfVV9rKq2VtV24APAvVX158B9wPvasD3A3a19sG3T9t9bVTXSqiVJg63mOvq/AT6SZI6FNfj9rX8/8MbW/xHgltWVKElajSFLN/+nqr4JfLO1HwOuOM2YXwHvH0FtkqQR8M5YSeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS55YM+iSvTvKdJN9P8nCSj7f+i5M8kGQuyZeSnNf6X9W259r+7eM9BUnS7zNkRv8/wNVV9VbgUuCaJFcCnwRurao3AU8De9v4vcDTrf/WNk6SNCFLBn0teK5tntteBVwN3NX6DwC7W3tX26bt35kkI6tYkrQsg9bok5yT5ChwCjgE/BfwTFW90IacALa09hbgOEDb/yzwxtN850yS2SSz8/PzqzsLSdIZDQr6qvpNVV0KbAWuAN6y2gNX1b6qmq6q6ampqdV+nSTpDJZ11U1VPQPcB7wN2JhkQ9u1FTjZ2ieBbQBt/xuAn42kWknSsg256mYqycbWfg3wTuAYC4H/vjZsD3B3ax9s27T991ZVjbJoSdJwG5YewmbgQJJzWPgfw51V9bUkjwBfTPL3wIPA/jZ+P/DPSeaAnwMfGEPdkqSBlgz6qnoIuOw0/Y+xsF7/8v5fAe8fSXWSpFXzzlhJ6pxBL0mdM+glqXMGvSR1zqCXpM4NubxS0lnOp1Wtb87oJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXNLBn2SbUnuS/JIkoeT3Nz6L0xyKMmj7f2C1p8ktyWZS/JQksvHfRKSpDMbMqN/AfjrqroEuBK4KcklwC3A4araARxu2wDXAjvaawa4feRVS5IGWzLoq+qJqvpea/8SOAZsAXYBB9qwA8Du1t4F3FEL7gc2Jtk88solSYMsa40+yXbgMuABYFNVPdF2PQlsau0twPFFHzvR+iRJEzA46JO8Fvgy8OGq+sXifVVVQC3nwElmkswmmZ2fn1/ORyVJyzAo6JOcy0LIf76qvtK6n3pxSaa9n2r9J4Ftiz6+tfW9RFXtq6rpqpqemppaaf2SpCUMueomwH7gWFV9atGug8Ce1t4D3L2o/4Z29c2VwLOLlngkSa+wDQPGXAV8CPhBkqOt72+BTwB3JtkLPA5c3/bdA1wHzAHPAzeOtGJJ0rIsGfRV9W0gZ9i98zTjC7hplXVJkkbEO2MlqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHVuyG/GSupAzvSDoOqeM3pJ6pxBL0mdM+glqXMGvSR1zqCXpM4tGfRJPpfkVJIfLuq7MMmhJI+29wtaf5LclmQuyUNJLh9n8ZKkpQ2Z0f8TcM3L+m4BDlfVDuBw2wa4FtjRXjPA7aMpU5K0UksGfVV9C/j5y7p3AQda+wCwe1H/HbXgfmBjks2jKlaStHwrXaPfVFVPtPaTwKbW3gIcXzTuROuTJE3Iqv8YW1UF1HI/l2QmyWyS2fn5+dWWIUk6g5UG/VMvLsm091Ot/ySwbdG4ra3vd1TVvqqarqrpqampFZYhSVrKSoP+ILCntfcAdy/qv6FdfXMl8OyiJR5J0gQs+VCzJF8A/hS4KMkJ4O+ATwB3JtkLPA5c34bfA1wHzAHPAzeOoWZJ0jIsGfRV9cEz7Np5mrEF3LTaoiRJo+OdsZLUOYNekjpn0EtS5wx6SeqcPyUoqSur+cnEWvatn+uDM3pJ6pxBL0mdM+glqXMGvSR1zj/GSuvMav7YqLOTM3pJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6Seqcd8ZKE+DdrXolOaOXpM4Z9JLUOYNekjpn0EtS58YS9EmuSfLjJHNJbhnHMaS1IFnZS3oljTzok5wD/ANwLXAJ8MEkl4z6OJKkYcYxo78CmKuqx6rq18AXgV1jOI40EiudlTsz13oxjuvotwDHF22fAP5kDMeRXsLg1Wqt9N+hqtHWMWoTu2EqyQww0zafS/LjFX7VRcBPR1PVmuZ59sXz7EgysfP8oyGDxhH0J4Fti7a3tr6XqKp9wL7VHizJbFVNr/Z71jrPsy+eZ1/W+nmOY43+u8COJBcnOQ/4AHBwDMeRJA0w8hl9Vb2Q5C+BfwfOAT5XVQ+P+jiSpGHGskZfVfcA94zju09j1cs/64Tn2RfPsy9r+jxTa/3PxZKkVfERCJLUuXUd9GfDoxaSfC7JqSQ/nHQt45RkW5L7kjyS5OEkN0+6pnFI8uok30ny/XaeH590TeOU5JwkDyb52qRrGZckP0nygyRHk8xOup7TWbdLN+1RC/8JvJOFm7K+C3ywqh6ZaGEjluQdwHPAHVX1x5OuZ1ySbAY2V9X3krwOOALs7vCfZ4Dzq+q5JOcC3wZurqr7J1zaWCT5CDANvL6q3jPpesYhyU+A6apas/cLrOcZ/VnxqIWq+hbw80nXMW5V9URVfa+1fwkcY+Eu667Ugufa5rnttT5nW0tIshV4N/DZSddytlvPQX+6Ry10FwxnoyTbgcuAByZbyXi05YyjwCngUFV1eZ7Ap4GPAr+ddCFjVsA3khxpd/yvOes56NWhJK8Fvgx8uKp+Mel6xqGqflNVl7Jw1/gVSbpbkkvyHuBUVR2ZdC2vgLdX1eUsPLH3prbcuqas56Af9KgFrR9tzfrLwOer6iuTrmfcquoZ4D7gmknXMgZXAe9t69dfBK5O8i+TLWk8qupkez8FfJWFZeU1ZT0HvY9a6Ej7I+V+4FhVfWrS9YxLkqkkG1v7NSxcTPCjyVY1elX1saraWlXbWfhv896q+osJlzVySc5vFw+Q5HzgXcCau0Ju3QZ9Vb0AvPiohWPAnT0+aiHJF4D/AN6c5ESSvZOuaUyuAj7EwszvaHtdN+mixmAzcF+Sh1iYrByqqm4vPTwLbAK+neT7wHeAf6uqr0+4pt+xbi+vlCQNs25n9JKkYQx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6979K7scU+IhypQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binsize = 0.25\n",
    "bins = np.arange(0, 5.5, binsize)\n",
    "plt.hist(y_test['prediction'].values, bins = bins, color = 'b')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
